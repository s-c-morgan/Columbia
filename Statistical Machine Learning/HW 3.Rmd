---
title: "HW 3"
author: "Carlyle Morgan"
date: "3/25/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(quantmod)
library(stringr)
library(plyr)
library(MASS)
library(nnet)
library(e1071)
set.seed(2)
```

# Problem 1

```{r}
digit_5<-read.csv("train.5.txt")
digit_5$digit<-"5"
digit_6<-read.csv("train.6.txt")
digit_6$digit<-"6"
colnames(digit_5)<-NA
colnames(digit_6)<-NA
digits<-rbind(digit_5,digit_6)
colnames(digits)<-seq(1,256,1)
colnames(digits)[257]<-"digit"
test_ind<-sample(seq(1,nrow(digits)), nrow(digits)/5)
digits$digit<-as.factor(digits$digit)

digits_train<-digits[-test_ind,]
digits_test<-digits[test_ind,]

linearcrossval <- tune(svm, digit~., data = digits_train, scale = FALSE, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1,5,10,100)))

summary(linearcrossval$best.model)

### Because cross-validation with radial kernels is computationally intensive, I ran this 
### process once and then exported the results to a csv

radialperformances<-read.csv("radialperformances.csv")

```

## 1.

### a.

```{r}
plot(error~log10(cost), linearcrossval$performances, type = "l", main = "Margin Parameter vs Error for Linear Kernel", col = "red")

```

### b.

```{r}
x<-c("0.001", "0.01", "0.1", "1", "5", "10", "100")
y<-c("0.001", "0.01", "0.1", "1", "5", "10", "100")
data <- expand.grid(X=x, Y=y)
data$Z<-round(radialperformances$error,3)

ggplot(data, aes(X, Y, fill= Z)) + 
  geom_tile()+
  geom_text(aes(label = Z), color = "white", size = 4)+
  labs(title = "Radial Basis Kernel Cross Validation Results", x = "Cost", y = "Gamma")
```

## 2.

From our earlier results, we should run a linear kernel SVM with cost equal to 0.01 and a radial kernel with cost equal to 1 and Gamma equal to 0.01.

```{r}
linsvm<-svm(digit~., digits_train, kernel = "linear", cost = 0.01, scale = FALSE)
radialsvm<-svm(digit~., digits_train, kernel = "radial", cost = 1, Gamma = 0.01, scale = FALSE)

sum(ifelse(predict(linsvm, digits_test)==digits_test$digit,0,1))/nrow(digits_test)
sum(ifelse(predict(radialsvm, digits_test)==digits_test$digit,0,1))/nrow(digits_test)
```

The radial kernel has a smaller test error under zero-one loss than the linear kernel, so a RBF kernel seems more ideal for this scenario.



# Problem 2

\pagebreak

## v.

```{r}
radialdata<-read.csv("HW3Problem2.csv")
pr2pt51<-svm(as.factor(class)~., data = radialdata, kernel = "radial", cost = 1, Gamma = 0.05, scale = FALSE)
plot(pr2pt51, radialdata)

pr2pt52<-svm(as.factor(class)~., data = radialdata, kernel = "radial", cost = 5, Gamma = 0.05, scale = FALSE)
plot(pr2pt52, radialdata)

pr2pt53<-svm(as.factor(class)~., data = radialdata, kernel = "radial", cost = 0.01, Gamma = 0.02, scale = FALSE)
plot(pr2pt53, radialdata)

pr2pt54<-svm(as.factor(class)~., data = radialdata, kernel = "radial", cost = 0.5, Gamma = 0.05, scale = FALSE)
plot(pr2pt54, radialdata)
```

# Problem 3

## i.

```{r}
f1<-function(x){
  return(max(1-x,0))
}
xes<-seq(-1,3,.01)
yes<-sapply(xes,f1)
plot(xes,yes, type = "l", xlab = "z", ylab = "g(z)")

```

\pagebreak


## iii.

```{r}
svmdata <- read.csv("svmdata.csv")
x1<-svmdata$x1
x2<-svmdata$x2
y<-svmdata$y
w1<-0
w2<-0
c<-0
lambda<-0.25

svmgraddesc<-function(y,x1,x2, lambda, epsilon, maxiter, w1_0,w2_0,c_0){
  w1<-w1_0
  w2<-w2_0
  c<-c_0
  convergence <- FALSE
  iterations <- 1
  while(convergence == FALSE & iterations < maxiter){
    eta<-1/(iterations*lambda)
    gradient_i<-matrix(NA, length(x1),3)
    for (i in 1:length(x1)){
      if((y[i]*(w1*x1[i]+w2*x2[i]+c))<1){
        gradient_i[i,]<- c(-y[i], -y[i]*x1[i]+lambda*w1, -y[i]*x2[i]+lambda*w2)
      }
      if((y[i]*(w1*x1[i]+w2*x2[i]+c))>1){
         gradient_i[i,]<- c(0, lambda*w1, lambda*w2)
        }
    }
    c<-c-(eta*mean(gradient_i[,1])) 
    w1<-w1-(eta*mean(gradient_i[,2]))
    w2<-w2-(eta*mean(gradient_i[,3]))
    if(abs(mean(gradient_i[,1]))+abs(mean(gradient_i[,2]))+abs(mean(gradient_i[,3]))<epsilon){
      convergence<-TRUE
    }
    iterations<-iterations+1
  }
  return(c(c,w1,w2))
}

test_svm<-svmgraddesc(y,x1,x2,0.3,0.001,10000,0,0,0)
```

```{r}
c<-test_svm[1]
w1<-test_svm[2]
w2<-test_svm[3]

plot(svmdata$x1, svmdata$x2)
abline(-c/w2, -w1/w2, col = "blue", xlab ="x1", ylab = "x2")

## Coefficients:
c(c,w1,w2)
```